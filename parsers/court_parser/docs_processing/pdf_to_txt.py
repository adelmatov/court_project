#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF —Å—É–¥–µ–±–Ω—ã—Ö –∞–∫—Ç–æ–≤ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç–æ—Ä–æ–Ω –¥–µ–ª–∞.
–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è NER/LLM –æ–±—Ä–∞–±–æ—Ç–∫–∏.
"""

import re
import sys
import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict

try:
    import fitz  # PyMuPDF
except ImportError:
    print("‚ùå –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ PyMuPDF: pip install pymupdf")
    sys.exit(1)


# ============================================================
# –ö–û–ù–°–¢–ê–ù–¢–´
# ============================================================

PREPOSITIONS = {
    '–≤', '–Ω–∞', '–ø–æ', '—Å', '—Å–æ', '–∫', '–∫–æ', '–æ', '–æ–±', '–æ—Ç', '–¥–æ', '–∏–∑', '–∑–∞',
    '–¥–ª—è', '–ø—Ä–∏', '–±–µ–∑', '–ø–æ–¥', '–Ω–∞–¥', '–º–µ–∂–¥—É', '—á–µ—Ä–µ–∑', '–ø–µ—Ä–µ–¥', '–ø–æ—Å–ª–µ',
    '–∏', '–∞', '–Ω–æ', '–∏–ª–∏', '–ª–∏–±–æ', '—á—Ç–æ', '–∫–∞–∫', '—Ç–∞–∫', '—á–µ–º',
    '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä–∞—è', '–∫–æ—Ç–æ—Ä–æ–µ', '–∫–æ—Ç–æ—Ä—ã–µ', '–∫–æ—Ç–æ—Ä–æ–≥–æ', '–∫–æ—Ç–æ—Ä–æ–π', '–∫–æ—Ç–æ—Ä—ã–º',
}

NUMERALS = {
    '–æ–¥–∏–Ω', '–æ–¥–Ω–∞', '–æ–¥–Ω–æ', '–æ–¥–Ω–æ–≥–æ', '–æ–¥–Ω–æ–π', '–æ–¥–Ω–æ–º—É', '–æ–¥–Ω–∏–º', '–æ–¥–Ω–æ–º',
    '–¥–≤–∞', '–¥–≤–µ', '–¥–≤—É—Ö', '–¥–≤—É–º', '–¥–≤—É–º—è',
    '—Ç—Ä–∏', '—Ç—Ä—ë—Ö', '—Ç—Ä–µ—Ö', '—Ç—Ä—ë–º', '—Ç—Ä–µ–º', '—Ç—Ä–µ–º—è',
    '—á–µ—Ç—ã—Ä–µ', '—á–µ—Ç—ã—Ä—ë—Ö', '—á–µ—Ç—ã—Ä–µ—Ö', '—á–µ—Ç—ã—Ä—ë–º', '—á–µ—Ç—ã—Ä–µ–º', '—á–µ—Ç—ã—Ä—å–º—è',
    '–ø—è—Ç—å', '—à–µ—Å—Ç—å', '—Å–µ–º—å', '–≤–æ—Å–µ–º—å', '–¥–µ–≤—è—Ç—å', '–¥–µ—Å—è—Ç—å',
    '–ø—è—Ç–∏', '—à–µ—Å—Ç–∏', '—Å–µ–º–∏', '–≤–æ—Å—å–º–∏', '–¥–µ–≤—è—Ç–∏', '–¥–µ—Å—è—Ç–∏',
    '–æ–¥–∏–Ω–Ω–∞–¥—Ü–∞—Ç—å', '–¥–≤–µ–Ω–∞–¥—Ü–∞—Ç—å', '—Ç—Ä–∏–Ω–∞–¥—Ü–∞—Ç—å', '—á–µ—Ç—ã—Ä–Ω–∞–¥—Ü–∞—Ç—å', '–ø—è—Ç–Ω–∞–¥—Ü–∞—Ç—å',
    '–Ω–µ—Å–∫–æ–ª—å–∫–æ', '–Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö', '–Ω–µ—Å–∫–æ–ª—å–∫–∏–º', '–Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏',
    '–º–Ω–æ–≥–æ', '–º–Ω–æ–≥–∏—Ö', '–º–Ω–æ–≥–∏–º', '–º–Ω–æ–≥–∏–º–∏',
}

ABBREVIATIONS = {
    '–†–ö', '–†–§', '–ö–ó', '–°–°–°–†', '–°–ù–ì',
    '–ì–ö', '–ì–ü–ö', '–£–ö', '–£–ü–ö', '–ö–æ–ê–ü', '–ù–ö', '–¢–ö', '–ó–ö', '–ñ–ö', '–°–ö', '–ê–ü–ö',
    '–ê–ü–ü–ö', '–ê–û', '–¢–û–û', '–ò–ü', '–û–ê–û', '–ó–ê–û', '–û–û–û', '–ù–ê–û', '–ü–ê–û',
    '–†–ì–£', '–ì–£', '–ö–ì–£', '–†–ì–ö–ü', '–†–ì–ü', '–ì–ö–ü',
    '–ú–†–ü', '–ú–ó–ü', '–ë–ò–ù', '–ò–ò–ù', '–ù–î–°', '–ö–ü–ù',
    '–£–ì–î', '–î–ì–î', '–ö–ì–î',
}

CITIES = {
    '–ê–ª–º–∞—Ç—ã', '–ê—Å—Ç–∞–Ω–∞', '–®—ã–º–∫–µ–Ω—Ç', '–ö–∞—Ä–∞–≥–∞–Ω–¥–∞', '–ê–∫—Ç–æ–±–µ', '–¢–∞—Ä–∞–∑', '–ü–∞–≤–ª–æ–¥–∞—Ä',
    '–£—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫', '–°–µ–º–µ–π', '–ö–æ—Å—Ç–∞–Ω–∞–π', '–ü–µ—Ç—Ä–æ–ø–∞–≤–ª–æ–≤—Å–∫', '–ö—ã–∑—ã–ª–æ—Ä–¥–∞',
    '–ê—Ç—ã—Ä–∞—É', '–ê–∫—Ç–∞—É', '–£—Ä–∞–ª—å—Å–∫', '–¢–µ–º–∏—Ä—Ç–∞—É', '–¢—É—Ä–∫–µ—Å—Ç–∞–Ω', '–ö–æ–∫—à–µ—Ç–∞—É',
    '–¢–∞–ª–¥—ã–∫–æ—Ä–≥–∞–Ω', '–≠–∫–∏–±–∞—Å—Ç—É–∑', '–†—É–¥–Ω—ã–π', '–ñ–µ–∑–∫–∞–∑–≥–∞–Ω', '–ñ–∞–Ω–∞–æ–∑–µ–Ω', '–ë–∞–ª—Ö–∞—à',
    '–ö–µ–Ω—Ç–∞—É', '–°–∞—Ç–ø–∞–µ–≤', '–ö–∞—Å–∫–µ–ª–µ–Ω', '–ö–æ–Ω–∞–µ–≤',
}


# ============================================================
# DATACLASS –î–õ–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
# ============================================================

@dataclass
class CourtCase:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å—É–¥–µ–±–Ω–æ–≥–æ –¥–µ–ª–∞"""
    case_number: Optional[str] = None
    document_type: Optional[str] = None
    date: Optional[str] = None
    city: Optional[str] = None
    court: Optional[str] = None
    presiding_judge: Optional[str] = None
    judges: List[str] = None
    plaintiffs: List[str] = None
    defendants: List[str] = None
    secretary: Optional[str] = None
    
    def __post_init__(self):
        if self.judges is None:
            self.judges = []
        if self.plaintiffs is None:
            self.plaintiffs = []
        if self.defendants is None:
            self.defendants = []
    
    def to_dict(self) -> dict:
        return asdict(self)
    
    def to_json(self, indent: int = 2) -> str:
        return json.dumps(self.to_dict(), ensure_ascii=False, indent=indent)


# ============================================================
# –§–£–ù–ö–¶–ò–ò –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø –¢–ï–ö–°–¢–ê –ò–ó PDF
# ============================================================

def extract_text(pdf_path: Path) -> Tuple[str, int]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—ã—Ä–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF"""
    doc = fitz.open(pdf_path)
    pages_text = []
    
    for page in doc:
        text = page.get_text("text", sort=True)
        pages_text.append(text)
    
    doc.close()
    return "\n".join(pages_text), len(pages_text)


def remove_page_numbers(text: str) -> str:
    """–£–¥–∞–ª–µ–Ω–∏–µ –Ω–æ–º–µ—Ä–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü"""
    text = re.sub(r'^\s*\d{1,3}\s*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*[-‚Äî]\s*\d{1,3}\s*[-‚Äî]\s*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*(—Å—Ç—Ä\.?|—Å—Ç—Ä–∞–Ω–∏—Ü–∞)\s*\d{1,3}\s*$', '', text, flags=re.MULTILINE | re.IGNORECASE)
    text = re.sub(r'^\s*\d{1,3}\s*(–∏–∑|/)\s*\d{1,3}\s*$', '', text, flags=re.MULTILINE)
    return text


def fix_word_hyphenation(text: str) -> str:
    """–°–∫–ª–µ–π–∫–∞ —Å–ª–æ–≤ —Å –ø–µ—Ä–µ–Ω–æ—Å–æ–º"""
    text = re.sub(r'([–∞-—è—ë–ê-–Ø–Å”ô“ì“õ“£”©“±“Ø“ª—ñ”ò“í“ö“¢”®“∞“Æ“∫–Üa-zA-Z])-\n([–∞-—è—ë”ô“ì“õ“£”©“±“Ø“ª—ña-z])', r'\1\2', text)
    return text


def split_header_from_body(text: str) -> str:
    """–û—Ç–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–æ—á–Ω–æ–π —á–∞—Å—Ç–∏ (–¥–∞—Ç–∞ + –≥–æ—Ä–æ–¥) –æ—Ç –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
    cities_pattern = '|'.join(CITIES)
    
    pattern = rf'(\d{{1,2}}\s+(?:—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s+\d{{4}}\s+–≥–æ–¥–∞?\s*(?:‚Ññ[^\s]+\s+)?(?:–≥\.\s*|–≥–æ—Ä–æ–¥\s+)?(?:{cities_pattern}))\s+([–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë])'
    
    def replacer(match):
        header = match.group(1)
        next_char = match.group(2)
        return f"{header}\n\n{next_char}"
    
    text = re.sub(pattern, replacer, text, flags=re.IGNORECASE)
    return text


def split_signatures(text: str) -> str:
    """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ–¥–ø–∏—Å–µ–π —Å—É–¥–µ–π –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏"""
    text = re.sub(r'(\S)\s+(–°—É–¥—å[—è–∏])\s+', r'\1\n\n\2 ', text)
    text = re.sub(r'(\S)\s+(–ü—Ä–µ–¥—Å–µ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É—é—â\w*)\s+', r'\1\n\n\2 ', text)
    text = re.sub(
        r'([–∞-—è—ë“±“Ø“ì“õ“£”ô”©“ª—ñ]\s*)([–ê-–Ø–Å“∞“Æ“í“ö“¢”ò”®“∫–Ü]\.[–ê-–Ø–Å“∞“Æ“í“ö“¢”ò”®“∫–Ü]?\.\s*[–ê-–Ø–Å“∞“Æ“í“ö“¢”ò”®“∫–Ü–∞-—è—ë“±“Ø“ì“õ“£”ô”©“ª—ñ]+)',
        r'\1\n\2',
        text
    )
    return text


# ============================================================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò –î–õ–Ø –û–ë–†–ê–ë–û–¢–ö–ò –¢–ï–ö–°–¢–ê
# ============================================================

def is_simple_parenthetical(line: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ—Å—Ç–æ–≥–æ –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–∞ –≤ —Å–∫–æ–±–∫–∞—Ö"""
    line_stripped = line.strip()
    
    if not re.match(r'^\([^)]+\)\s*$', line_stripped):
        return False
    
    content = line_stripped[1:-1].strip()
    special_chars = ['‚Äì', '-', '‚Äî', ',', '¬´', '¬ª', '"', '"', ':', ';']
    for char in special_chars:
        if char in content:
            return False
    
    if re.match(r'^[–∞-—è—ë–ê-–Ø–Å”ô“ì“õ“£”©“±“Ø“ª—ñ”ò“í“ö“¢”®“∞“Æ“∫–Üa-zA-Z\s]+$', content):
        return True
    
    return False


def is_header_line(line: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
    line_stripped = line.strip()
    
    if not line_stripped:
        return True
    
    headers = [
        '–†–ï–®–ï–ù–ò–ï', '–û–ü–†–ï–î–ï–õ–ï–ù–ò–ï', '–ü–û–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï', '–ü–†–ò–ì–û–í–û–†',
        '–£–°–¢–ê–ù–û–í–ò–õ', '–£–°–¢–ê–ù–û–í–ò–õ–ê', '–£–°–¢–ê–ù–û–í–ò–õ–û',
        '–ü–û–°–¢–ê–ù–û–í–ò–õ', '–ü–û–°–¢–ê–ù–û–í–ò–õ–ê', '–ü–û–°–¢–ê–ù–û–í–ò–õ–û',
        '–†–ï–®–ò–õ', '–†–ï–®–ò–õ–ê', '–†–ï–®–ò–õ–û',
        '–û–ü–†–ï–î–ï–õ–ò–õ', '–û–ü–†–ï–î–ï–õ–ò–õ–ê', '–û–ü–†–ï–î–ï–õ–ò–õ–û',
        '–†–ï–ó–û–õ–Æ–¢–ò–í–ù–ê–Ø –ß–ê–°–¢–¨', '–ú–û–¢–ò–í–ò–†–û–í–û–ß–ù–ê–Ø –ß–ê–°–¢–¨',
    ]
    
    if line_stripped.upper() in headers:
        return True
    
    if re.match(r'^[–ê-–Ø–Å](\s+[–ê-–Ø–Å]){3,}[:\s]*$', line_stripped):
        return True
    
    if is_simple_parenthetical(line_stripped):
        return True
    
    return False


def is_abbreviation(line: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞, –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ —Å –∏–∑–≤–µ—Å—Ç–Ω–æ–π –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã"""
    line_stripped = line.strip()
    
    for abbr in ABBREVIATIONS:
        if line_stripped.startswith(abbr + ' ') or line_stripped.startswith(abbr + '.') or line_stripped.startswith(abbr + ',') or line_stripped == abbr:
            return True
    
    if re.match(r'^[–ê-–Ø–ÅA-Z]{2,6}[.\s,)]', line_stripped):
        return True
    
    return False


def is_name_with_initials(line: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ —Ñ–∞–º–∏–ª–∏–µ–π —Å –∏–Ω–∏—Ü–∏–∞–ª–∞–º–∏"""
    line_stripped = line.strip()
    
    if re.match(r'^[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ][–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+\s+[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]\.[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]?\.?\s*', line_stripped):
        return True
    
    if re.match(r'^[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]\.[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]?\.\s*[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+', line_stripped):
        return True
    
    return False


def is_parenthetical_explanation(line: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—è—Å–Ω–µ–Ω–∏—è –≤ —Å–∫–æ–±–∫–∞—Ö"""
    line_stripped = line.strip()
    
    if re.match(r'^\(–¥–∞–ª–µ–µ\s*[-‚Äì‚Äî]', line_stripped, re.IGNORECASE):
        return True
    
    if re.match(r'^\([^)]+\)\s*$', line_stripped):
        content = line_stripped[1:-1]
        special_chars = ['‚Äì', '-', '‚Äî', ',', '¬´', '¬ª', '"', '"', ':', ';']
        for char in special_chars:
            if char in content:
                return True
    
    return False


def is_list_item(line: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–∞ —Å–ø–∏—Å–∫–∞"""
    line_stripped = line.strip()
    
    if re.match(r'^\d+[.\)]\s', line_stripped):
        return True
    
    if re.match(r'^[–∞-—èa-z][.\)]\s', line_stripped):
        return True
    
    if re.match(r'^[-‚Ä¢*]\s', line_stripped):
        return True
    
    return False


def is_date_line(line: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –¥–∞—Ç–æ–π"""
    line_stripped = line.strip()
    
    if re.match(r'^\d{1,2}\s+(—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s+\d{4}', line_stripped, re.IGNORECASE):
        return True
    
    return False


def is_standalone_metadata_line(line: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
    line_stripped = line.strip()
    
    cities_pattern = '|'.join(CITIES)
    if re.match(rf'^\d{{1,2}}\s+(?:—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s+\d{{4}}\s+(?:–≥–æ–¥–∞?\s+)?(?:‚Ññ[^\s]+\s+)?(?:–≥\.\s*|–≥–æ—Ä–æ–¥\s+)?(?:{cities_pattern})\s*$', line_stripped, re.IGNORECASE):
        return True
    
    return False


def is_signature_line(line: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–ø–∏—Å–∏"""
    line_stripped = line.strip()
    
    if re.match(r'^( –ü—Ä–µ–¥—Å–µ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É—é—â|–°—É–¥—å[—è–∏–µ—ë—é]|–°–µ–∫—Ä–µ—Ç–∞—Ä—å)', line_stripped, re.IGNORECASE):
        return True
    
    if re.match(r'^[–ê-–Ø–Å]\.[–ê-–Ø–Å]\.\s+[–ê-–Ø–Å][–∞-—è—ë]+\s*$', line_stripped):
        return True
    
    if re.match(r'^[–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å]\.[–ê-–Ø–Å]\.\s*$', line_stripped):
        return True
    
    return False


def is_initials(line: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–æ–≤ –≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫–∏"""
    line_stripped = line.strip()
    if re.match(r'^[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]\.[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]?\.?', line_stripped):
        return True
    return False


def starts_with_adjective_or_participle(line: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–≥–æ/–ø—Ä–∏—á–∞—Å—Ç–∏—è –≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫–∏"""
    line_stripped = line.strip()
    
    endings = (
        '–æ–≥–æ', '–µ–≥–æ', '–æ–π', '–µ–π', '—ã—Ö', '–∏—Ö', '–æ–º—É', '–µ–º—É',
        '—ã–º', '–∏–º', '—É—é', '—é—é', '–æ–µ', '–µ–µ', '–∞—è', '—è—è',
        '—ã–º–∏', '–∏–º–∏', '–Ω–æ–º', '–Ω–µ–º', '–Ω–æ–π', '–Ω–µ–π',
        '—â–µ–≥–æ', '–≤—à–µ–≥–æ', '–Ω–Ω–æ–≥–æ', '—Ç–æ–≥–æ', '–º–æ–≥–æ',
        '—â–µ–π', '–≤—à–µ–π', '–Ω–Ω–æ–π', '—â–∏—Ö', '–≤—à–∏—Ö', '–Ω–Ω—ã—Ö',
        '–Ω—ã–º–∏', '—Å–∫–∏–º', '—Å–∫–æ–π', '—Å–∫–æ–µ', '—Å–∫–∏—Ö', '—Å–∫–∞—è',
        '—ë–Ω–Ω—ã–π', '–µ–Ω–Ω—ã–π', '–∞–Ω–Ω—ã–π', '—è–Ω–Ω—ã–π',
        '–æ–≤–∞–Ω–Ω—ã–π', '—ë–≤–∞–Ω–Ω—ã–π', '–µ–≤–∞–Ω–Ω—ã–π',
    )
    
    pattern = r'^[–ê-–Ø–Å–∞-—è—ë“ö“í”ò“¢”®“∞“Æ“∫–Ü“õ“ì”ô“£”©“±“Ø“ª—ñ][–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+(' + '|'.join(endings) + r')\b'
    if re.match(pattern, line_stripped):
        return True
    
    return False


def starts_with_lowercase(line: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞, –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ —Å –º–∞–ª–µ–Ω—å–∫–æ–π –±—É–∫–≤—ã"""
    line_stripped = line.strip()
    if line_stripped and re.match(r'^[–∞-—è—ë”ô“ì“õ“£”©“±“Ø“ª—ña-z]', line_stripped):
        return True
    return False


def starts_with_digit(line: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞, –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ —Å —Ü–∏—Ñ—Ä—ã"""
    line_stripped = line.strip()
    return bool(re.match(r'^\d', line_stripped))


def ends_with_sentence_end(line: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞, –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –∫–æ–Ω—Ü–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
    line_stripped = line.strip()
    return bool(re.search(r'[.!?]\s*$', line_stripped))


def ends_with_colon(line: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞, –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –¥–≤–æ–µ—Ç–æ—á–∏–µ–º"""
    line_stripped = line.strip()
    return line_stripped.endswith(':')


def ends_with_preposition(line: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è –Ω–∞ –ø—Ä–µ–¥–ª–æ–≥/—Å–æ—é–∑"""
    line_stripped = line.strip().lower()
    words = line_stripped.split()
    if not words:
        return False
    last_word = re.sub(r'[^\w]', '', words[-1])
    return last_word in PREPOSITIONS


def ends_with_numeral(line: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è –Ω–∞ —á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–µ"""
    line_stripped = line.strip().lower()
    words = line_stripped.split()
    if not words:
        return False
    last_word = re.sub(r'[^\w]', '', words[-1])
    return last_word in NUMERALS


def ends_with_digit(line: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è –Ω–∞ —Ü–∏—Ñ—Ä—É"""
    line_stripped = line.strip()
    return bool(re.search(r'\d\s*$', line_stripped))


def ends_with_number_dash(line: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è –Ω–∞ —Ü–∏—Ñ—Ä—É —Å –¥–µ—Ñ–∏—Å–æ–º (–¥–∏–∞–ø–∞–∑–æ–Ω): 148-"""
    line_stripped = line.strip()
    return bool(re.search(r'\d[-‚Äì‚Äî]\s*$', line_stripped))


def ends_with_open_quote(line: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–∑–∞–∫—Ä—ã—Ç–æ–π –∫–∞–≤—ã—á–∫–∏"""
    line_stripped = line.strip()
    open_quotes = line_stripped.count('¬´') + line_stripped.count('"')
    close_quotes = line_stripped.count('¬ª') + line_stripped.count('"')
    return open_quotes > close_quotes


def ends_with_open_paren(line: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–∑–∞–∫—Ä—ã—Ç–æ–π —Å–∫–æ–±–∫–∏"""
    line_stripped = line.strip()
    open_parens = line_stripped.count('(')
    close_parens = line_stripped.count(')')
    return open_parens > close_parens


def ends_with_number_paren(line: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è –Ω–∞ —Ü–∏—Ñ—Ä—É —Å–æ —Å–∫–æ–±–∫–æ–π"""
    line_stripped = line.strip()
    return bool(re.search(r'\d\)\s*$', line_stripped))


def ends_with_dash(line: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è –Ω–∞ —Ç–∏—Ä–µ"""
    line_stripped = line.strip()
    return bool(re.search(r'\s[-‚Äî]\s*$', line_stripped))


def ends_with_word(line: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞, –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –Ω–∞ —Å–ª–æ–≤–æ"""
    line_stripped = line.strip()
    if not line_stripped:
        return False
    return bool(re.search(r'[–∞-—è—ë–ê-–Ø–Å”ô“ì“õ“£”©“±“Ø“ª—ñ”ò“í“ö“¢”®“∞“Æ“∫–Üa-zA-Z]\s*$', line_stripped))


def should_force_merge(current_line: str, next_line: str) -> bool:
    """–£—Å–ª–æ–≤–∏—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π —Å–∫–ª–µ–π–∫–∏"""
    current = current_line.strip()
    next_stripped = next_line.strip()
    
    if not current or not next_stripped:
        return False
    
    if starts_with_lowercase(next_stripped):
        return True
    
    if ends_with_number_dash(current):
        return True
    
    if ends_with_preposition(current):
        return True
    
    if is_abbreviation(next_stripped):
        if not ends_with_sentence_end(current):
            return True
    
    if is_name_with_initials(next_stripped):
        if not ends_with_sentence_end(current):
            return True
    
    if is_parenthetical_explanation(next_stripped):
        return True
    
    if next_stripped.startswith('(') and not is_simple_parenthetical(next_stripped):
        return True
    
    if is_initials(next_stripped):
        return True
    
    if starts_with_adjective_or_participle(next_stripped):
        return True
    
    if ends_with_numeral(current):
        return True
    
    if ends_with_digit(current) and not ends_with_sentence_end(current):
        return True
    
    if ends_with_open_quote(current):
        return True
    
    if ends_with_open_paren(current):
        return True
    
    if ends_with_number_paren(current):
        return True
    
    if ends_with_dash(current):
        return True
    
    if current.endswith(','):
        if not is_list_item(next_stripped) and not is_signature_line(next_stripped):
            return True
    
    if ends_with_word(current) and not ends_with_sentence_end(current) and not ends_with_colon(current):
        if not is_header_line(next_stripped) and not is_list_item(next_stripped) and not is_signature_line(next_stripped):
            new_sentence_starters = [
                '–í —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏', '–ù–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏', '–ü–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º', '–ü—Ä–∏ —ç—Ç–æ–º',
                '–°–æ–≥–ª–∞—Å–Ω–æ ', '–†—É–∫–æ–≤–æ–¥—Å—Ç–≤—É—è—Å—å ', '–ó–∞—Å–ª—É—à–∞–≤ ', '–†–∞—Å—Å–º–æ—Ç—Ä–µ–≤ ',
                '–ò—Å—Ç–µ—Ü ', '–û—Ç–≤–µ—Ç—á–∏–∫ ', '–°—É–¥ ', '–°—É–¥–µ–±–Ω–∞—è ', '–°—É–¥—å—è ',
                '–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–π ', '–ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π ', '–ù–∞—Å—Ç–æ—è—â–µ–µ ',
                '–í—ã—à–µ—É–∫–∞–∑–∞–Ω–Ω–æ–µ ', '–£–∫–∞–∑–∞–Ω–Ω–æ–µ ', '–†–µ—à–µ–Ω–∏–µ–º ',
            ]
            is_new_sentence = any(next_stripped.startswith(s) for s in new_sentence_starters)
            
            if not is_new_sentence:
                return True
    
    return False


def should_not_merge(current_line: str, next_line: str) -> bool:
    """–£—Å–ª–æ–≤–∏—è –∑–∞–ø—Ä–µ—Ç–∞ —Å–∫–ª–µ–π–∫–∏"""
    current = current_line.strip()
    next_stripped = next_line.strip()
    
    if not current or not next_stripped:
        return True
    
    if is_header_line(current):
        return True
    
    if is_header_line(next_stripped):
        return True
    
    if is_signature_line(current) and is_signature_line(next_stripped):
        return True
    
    if is_list_item(next_stripped):
        return True
    
    if is_standalone_metadata_line(next_stripped):
        return True
    
    return False


def should_merge(current_line: str, next_line: str) -> bool:
    """–û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–∫–ª–µ–π–∫–∏"""
    current = current_line.strip()
    next_stripped = next_line.strip()
    
    if should_not_merge(current_line, next_line):
        return False
    
    if should_force_merge(current_line, next_line):
        return True
    
    if re.search(r'[–∞-—è—ëa-z0-9,;]$', current):
        if re.match(r'^[–∞-—è—ëa-z0-9]', next_stripped):
            return True
    
    if re.search(r'[–∞-—è—ë–ê-–Ø–Åa-zA-Z]$', current):
        if re.match(r'^\d', next_stripped):
            return True
    
    if re.search(r'\d$', current):
        if re.match(r'^[–∞-—è—ëa-z]', next_stripped):
            return True
    
    if re.match(r'^[)\]¬ª"]', next_stripped):
        return True
    
    return False


def get_next_non_empty_line(lines: list, start_index: int) -> Tuple[int, str]:
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ª–µ–¥—É—é—â—É—é –Ω–µ–ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –∏ –µ—ë –∏–Ω–¥–µ–∫—Å"""
    i = start_index + 1
    while i < len(lines):
        if lines[i].strip():
            return i, lines[i]
        i += 1
    return -1, ""


def merge_lines(text: str) -> str:
    """–°–∫–ª–µ–π–∫–∞ —Å—Ç—Ä–æ–∫ —Å –ø—Ä–æ–ø—É—Å–∫–æ–º –ø—É—Å—Ç—ã—Ö"""
    lines = text.split('\n')
    result = []
    i = 0
    
    while i < len(lines):
        current_line = lines[i]
        current_stripped = current_line.strip()
        
        if not current_stripped:
            result.append(current_line)
            i += 1
            continue
        
        next_idx, next_line = get_next_non_empty_line(lines, i)
        
        while next_idx != -1 and should_merge(current_line, next_line):
            next_stripped = next_line.strip()
            current_line = current_line.rstrip() + ' ' + next_stripped
            i = next_idx
            next_idx, next_line = get_next_non_empty_line(lines, i)
        
        result.append(current_line)
        i += 1
    
    return '\n'.join(result)


def normalize_whitespace(text: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤"""
    text = re.sub(r'[ \t]+', ' ', text)
    text = '\n'.join(line.strip() for line in text.split('\n'))
    text = re.sub(r'\n{3,}', '\n\n', text)
    text = text.strip()
    return text


def process_text(raw_text: str) -> str:
    """–ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
    text = raw_text
    text = remove_page_numbers(text)
    text = fix_word_hyphenation(text)
    text = merge_lines(text)
    text = split_header_from_body(text)
    text = split_signatures(text)
    text = normalize_whitespace(text)
    return text


# ============================================================
# –§–£–ù–ö–¶–ò–ò –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø –î–ê–ù–ù–´–• (NER)
# ============================================================

def normalize_name(name: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–º–µ–Ω–∏ (—É–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤, –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫)"""
    name = re.sub(r'\s+', ' ', name)
    name = name.strip()
    # –£–±–∏—Ä–∞–µ–º —Ç–æ—á–∫—É –≤ –∫–æ–Ω—Ü–µ –µ—Å–ª–∏ –µ—Å—Ç—å
    name = re.sub(r'\.$', '', name)
    return name


def extract_case_number(text: str) -> Optional[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–º–µ—Ä–∞ –¥–µ–ª–∞"""
    patterns = [
        r'‚Ññ\s*(\d{4}-\d{2}-\d{2}-\d[–∞-—èa-z]*/\d+)',
        r'–¥–µ–ª–æ\s*‚Ññ?\s*(\d{4}-\d{2}-\d{2}-\d[–∞-—èa-z]*/\d+)',
        r'‚Ññ\s*([^\s]+/\d+)',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            return match.group(1)
    return None


def extract_document_type(text: str) -> Optional[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    patterns = [
        (r'\b–û\s*–ü\s*–†\s*–ï\s*–î\s*–ï\s*–õ\s*–ï\s*–ù\s*–ò\s*–ï\b', '–û–ü–†–ï–î–ï–õ–ï–ù–ò–ï'),
        (r'\b–ü\s*–û\s*–°\s*–¢\s*–ê\s*–ù\s*–û\s*–í\s*–õ\s*–ï\s*–ù\s*–ò\s*–ï\b', '–ü–û–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï'),
        (r'\b–†\s*–ï\s*–®\s*–ï\s*–ù\s*–ò\s*–ï\b', '–†–ï–®–ï–ù–ò–ï'),
        (r'\b–ü\s*–†\s*–ò\s*–ì\s*–û\s*–í\s*–û\s*–†\b', '–ü–†–ò–ì–û–í–û–†'), (r'\b–û–ü–†–ï–î–ï–õ–ï–ù–ò–ï\b', '–û–ü–†–ï–î–ï–õ–ï–ù–ò–ï'),
        (r'\b–ü–û–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï\b', '–ü–û–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï'),
        (r'\b–†–ï–®–ï–ù–ò–ï\b', '–†–ï–®–ï–ù–ò–ï'),
        (r'\b–ü–†–ò–ì–û–í–û–†\b', '–ü–†–ò–ì–û–í–û–†'),
    ]
    
    for pattern, doc_type in patterns:
        if re.search(pattern, text, re.IGNORECASE):
            return doc_type
    return None


def extract_date(text: str) -> Optional[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    pattern = r'(\d{1,2})\s+(—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s+(\d{4})\s+–≥–æ–¥–∞?'
    match = re.search(pattern, text, re.IGNORECASE)
    if match:
        return f"{match.group(1)} {match.group(2)} {match.group(3)} –≥–æ–¥–∞"
    return None


def extract_city(text: str) -> Optional[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥–æ—Ä–æ–¥–∞"""
    cities_pattern = '|'.join(CITIES)
    pattern = rf'(?:–≥\.\s*|–≥–æ—Ä–æ–¥\s+)({cities_pattern})'
    match = re.search(pattern, text, re.IGNORECASE)
    if match:
        return match.group(1)
    
    # –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –≥–æ—Ä–æ–¥–∞
    for city in CITIES:
        if city in text:
            return city
    return None


def extract_court(text: str) -> Optional[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è —Å—É–¥–∞"""
    patterns = [
        r'(–°—É–¥–µ–±–Ω–∞—è\s+–∫–æ–ª–ª–µ–≥–∏—è\s+–ø–æ\s+(?:–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–º|–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–º|—É–≥–æ–ª–æ–≤–Ω—ã–º)\s+–¥–µ–ª–∞–º\s+(?:—Å—É–¥–∞\s+)?(?:–æ–±–ª–∞—Å—Ç–∏\s+)?[–ê-–Ø–Å–∞-—è—ë]+)',
        r'((?:—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω\w+\s+)?(?:–º–µ–∂—Ä–∞–π–æ–Ω–Ω\w+\s+)?(?:–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω\w+\s+)?—Å—É–¥[–∞]?\s+(?:–æ–±–ª–∞—Å—Ç–∏\s+)?[–ê-–Ø–Å–∞-—è—ë]+)',
        r'(–í–µ—Ä—Ö–æ–≤–Ω\w+\s+–°—É–¥\w*\s+–†–µ—Å–ø—É–±–ª–∏–∫–∏\s+–ö–∞–∑–∞—Ö—Å—Ç–∞–Ω)',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            return normalize_name(match.group(1))
    return None


def extract_presiding_judge(text: str) -> Optional[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–µ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É—é—â–µ–≥–æ —Å—É–¥—å–∏"""
    patterns = [
        # "–ø—Ä–µ–¥—Å–µ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É—é—â–µ–≥–æ —Å—É–¥—å–∏ –§–∞–º–∏–ª–∏—è –ò.–û."
        r'–ø—Ä–µ–¥—Å–µ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É—é—â(?:–µ–≥–æ|–µ–π|–∏–π)\s+—Å—É–¥—å[–∏—è–µ]\s+([–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+\s+[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]\.[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]?\.?)',
        # "–ü—Ä–µ–¥—Å–µ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É—é—â–∏–π —Å—É–¥—å—è: –§–∞–º–∏–ª–∏—è –ò.–û."
        r'–ü—Ä–µ–¥—Å–µ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É—é—â–∏–π\s+—Å—É–¥—å—è:?\s*([–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+\s*[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]?\.\s*[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]?\.?)',
        # "–ü—Ä–µ–¥—Å–µ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É—é—â–∏–π: –§–∞–º–∏–ª–∏—è –ò.–û."
        r'–ü—Ä–µ–¥—Å–µ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É—é—â\w*:?\s*([–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+\s*[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]?\.\s*[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]?\.?)',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            return normalize_name(match.group(1))
    return None


def extract_judges(text: str) -> List[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Å—É–¥–µ–π (–∫—Ä–æ–º–µ –ø—Ä–µ–¥—Å–µ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É—é—â–µ–≥–æ)"""
    judges = []
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —Å—É–¥–µ–π –≤ —Å–æ—Å—Ç–∞–≤–µ –∫–æ–ª–ª–µ–≥–∏–∏
    patterns = [
        # "—Å—É–¥–µ–π –§–∞–º–∏–ª–∏—è –ò.–û., –§–∞–º–∏–ª–∏—è –ò.–û."
        r'—Å—É–¥–µ–π\s+([–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+\s+[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]\.[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]?\.?)(?:,\s*([–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+\s+[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]\.[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]?\.?))?',
        # –°—É–¥—å–∏ –≤ –ø–æ–¥–ø–∏—Å–∏
        r'–°—É–¥—å[—è–∏]:?\s*([–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+\s*[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]?\.\s*[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]?\.?)',
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            if isinstance(match, tuple):
                for m in match:
                    if m:
                        judges.append(normalize_name(m))
            else:
                judges.append(normalize_name(match))
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    return list(dict.fromkeys(judges))


def extract_secretary(text: str) -> Optional[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ–∫—Ä–µ—Ç–∞—Ä—è —Å—É–¥–µ–±–Ω–æ–≥–æ –∑–∞—Å–µ–¥–∞–Ω–∏—è"""
    patterns = [
        r'(?:–ø—Ä–∏\s+)?—Å–µ–∫—Ä–µ—Ç–∞—Ä[–µ—ë—è]\s+(?:—Å—É–¥–µ–±–Ω–æ–≥–æ\s+–∑–∞—Å–µ–¥–∞–Ω–∏—è\s+)?([–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+\s+[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]\.[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]?\.?)',
        r'—Å–µ–∫—Ä–µ—Ç–∞—Ä[—å–µ—è]\s+([–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+\s+[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]\.[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]?\.?)',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            return normalize_name(match.group(1))
    return None


def extract_plaintiffs(text: str) -> List[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏—Å—Ç—Ü–æ–≤"""
    plaintiffs = []
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
    text_normalized = re.sub(r'\s+', ' ', text)
    
    patterns = [
        # "–ø–æ –∏—Å–∫—É –§–∞–º–∏–ª–∏—è –ò–º—è –û—Ç—á–µ—Å—Ç–≤–æ"
        r'–ø–æ\s+–∏—Å–∫—É\s+([–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+\s+[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+\s+[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+)',
        # "–ø–æ –∏—Å–∫—É –§–∞–º–∏–ª–∏—è –ò.–û."
        r'–ø–æ\s+–∏—Å–∫—É\s+([–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+\s+[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]\.[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]?\.?)',
        # "–∏—Å—Ç–µ—Ü –§–∞–º–∏–ª–∏—è –ò.–û."
        r'–∏—Å—Ç–µ—Ü\s+([–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+\s+[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]\.[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]?\.?)',
        # "–∏—Å—Ç—Ü–∞ –§–∞–º–∏–ª–∏—è –ò.–û."
        r'–∏—Å—Ç—Ü–∞\s+([–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+\s+[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]\.[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]?\.?)',
        # "–∏—Å—Ç—Ü–æ–º –§–∞–º–∏–ª–∏—è –ò.–û."
        r'–∏—Å—Ç—Ü–æ–º\s+([–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+\s+[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]\.[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]?\.?)',
        # "–ò—Å—Ç–µ—Ü –§–∞–º–∏–ª–∏—è –ò–º—è –û—Ç—á–µ—Å—Ç–≤–æ"
        r'[–ò–∏]—Å—Ç–µ—Ü\s+([–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+\s+[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+(?:\s+[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+)?)',
        # –î–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –ª–∏—Ü: "–ø–æ –∏—Å–∫—É –¢–û–û ¬´–ù–∞–∑–≤–∞–Ω–∏–µ¬ª"
        r'–ø–æ\s+–∏—Å–∫—É\s+((?:–¢–û–û|–ê–û|–û–û–û|–ò–ü)\s*[¬´"][^¬ª"]+[¬ª"])',
        # –î–ª—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –±–µ–∑ –∫–∞–≤—ã—á–µ–∫
        r'–ø–æ\s+–∏—Å–∫—É\s+([–ê-–Ø–Å][–ê-–Ø–Å–∞-—è—ë\s]+(?:–¢–û–û|–ê–û|–û–û–û|–ò–ü|–†–ì–£|–ì–£))',
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, text_normalized, re.IGNORECASE)
        for match in matches:
            name = normalize_name(match)
            if name and name not in plaintiffs and len(name) > 3:
                plaintiffs.append(name)
    
    return plaintiffs


def extract_defendants(text: str) -> List[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç—á–∏–∫–æ–≤"""
    defendants = []
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
    text_normalized = re.sub(r'\s+', ' ', text)
    
    patterns = [
        # –ß–°–ò —Å –ø–æ–ª–Ω—ã–º –∏–º–µ–Ω–µ–º
        r'–∫\s+—á–∞—Å—Ç–Ω–æ–º—É\s+—Å—É–¥–µ–±–Ω–æ–º—É\s+–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—é\s+([–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+\s+[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+\s+[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+)',
        # –ß–°–ò —Å –ò.–û.
        r'–∫\s+—á–∞—Å—Ç–Ω–æ–º—É\s+—Å—É–¥–µ–±–Ω–æ–º—É\s+–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—é\s+([–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+\s+[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]\.[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]?\.?)',
        # –ß–°–ò —Å–æ–∫—Ä–∞—â–µ–Ω–Ω–æ
        r'(?:–∫\s+)?–ß–°–ò\s+([–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+\s+[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]\.[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]?\.?)',
        r'–ß–°–ò\s+([–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+\s+[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+\s*[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]*)',
        # –£—á—Ä–µ–∂–¥–µ–Ω–∏–µ
        r'–∫\s+((?:—Ä–µ—Å–ø—É–±–ª–∏–∫–∞–Ω—Å–∫–æ–º—É\s+)?–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–º—É\s+—É—á—Ä–µ–∂–¥–µ–Ω–∏—é\s+[¬´"][^¬ª"]+[¬ª"])',
        r'–∫\s+(–†–ì–£\s+[¬´"][^¬ª"]+[¬ª"])',
        r'–∫\s+(–ì–£\s+[¬´"][^¬ª"]+[¬ª"])',
        # –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –ª–∏—Ü–∞
        r'–∫\s+((?:–¢–û–û|–ê–û|–û–û–û|–ò–ü)\s*[¬´"][^¬ª"]+[¬ª"])',
        # –û—Ç–≤–µ—Ç—á–∏–∫ —Ñ–∏–∑. –ª–∏—Ü–æ
        r'–∫\s+([–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+\s+[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+\s+[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+(?:—É|–æ–π)?)\s+(?:–æ–±\s+–æ—Å–ø–∞—Ä–∏–≤–∞–Ω–∏–∏|–æ\s+|–¥–∞–ª–µ–µ)',
        # –û—Ç–≤–µ—Ç—á–∏–∫ - –æ–±—â–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω
        r'–æ—Ç–≤–µ—Ç—á–∏–∫[–∞]?\s+([–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü–∞-—è—ë“õ“ì”ô“£”©“±“Ø“ª—ñ]+\s+[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]\.[–ê-–Ø–Å“ö“í”ò“¢”®“∞“Æ“∫–Ü]?\.?)',
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, text_normalized, re.IGNORECASE)
        for match in matches:
            name = normalize_name(match)
            if name and name not in defendants and len(name) > 3:
                defendants.append(name)
    
    return defendants


def extract_case_data(text: str) -> CourtCase:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å—É–¥–µ–±–Ω–æ–≥–æ –∞–∫—Ç–∞"""
    case = CourtCase()
    
    case.case_number = extract_case_number(text)
    case.document_type = extract_document_type(text)
    case.date = extract_date(text)
    case.city = extract_city(text)
    case.court = extract_court(text)
    case.presiding_judge = extract_presiding_judge(text)
    case.judges = extract_judges(text)
    case.secretary = extract_secretary(text)
    case.plaintiffs = extract_plaintiffs(text)
    case.defendants = extract_defendants(text)
    
    return case


# ============================================================
# –û–°–ù–û–í–ù–´–ï –§–£–ù–ö–¶–ò–ò –û–ë–†–ê–ë–û–¢–ö–ò
# ============================================================

def process_pdf(input_file: str, output_dir: str = None, save_txt: bool = True, save_json: bool = True) -> CourtCase:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ PDF —Ñ–∞–π–ª–∞:
    1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
    2. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ TXT (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (NER)
    4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    
    Args:
        input_file: –ø—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É
        output_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é - —Ä—è–¥–æ–º —Å PDF)
        save_txt: —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ —Ç–µ–∫—Å—Ç –≤ TXT
        save_json: —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –≤ JSON
    
    Returns:
        CourtCase: –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    """
    input_path = Path(input_file)
    
    if not input_path.exists():
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {input_path}")
        sys.exit(1)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    if output_dir:
        out_dir = Path(output_dir)
        out_dir.mkdir(parents=True, exist_ok=True)
    else:
        out_dir = input_path.parent
    
    print("=" * 60)
    print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞: {input_path.name}")
    print("=" * 60)
    
    # 1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF
    print("\nüîç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF...")
    raw_text, pages = extract_text(input_path)
    
    # 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    print("üìù –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞...")
    clean_text = process_text(raw_text)
    
    raw_lines = len(raw_text.split('\n'))
    clean_lines = len(clean_text.split('\n'))
    
    print(f"   ‚Ä¢ –°—Ç—Ä–∞–Ω–∏—Ü: {pages}")
    print(f"   ‚Ä¢ –°—Ç—Ä–æ–∫ –¥–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {raw_lines}")
    print(f"   ‚Ä¢ –°—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {clean_lines}")
    print(f"   ‚Ä¢ –°–∏–º–≤–æ–ª–æ–≤: {len(clean_text):,}")
    
    # 3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ TXT
    if save_txt:
        txt_path = out_dir / (input_path.stem + ".txt")
        txt_path.write_text(clean_text, encoding='utf-8')
        print(f"\n‚úÖ –¢–µ–∫—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {txt_path}")
    
    # 4. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (NER)
    print("\nüîé –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
    case_data = extract_case_data(clean_text)
    
    # 5. –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n" + "‚îÄ" * 40)
    print("üìã –ò–ó–í–õ–ï–ß–ï–ù–ù–´–ï –î–ê–ù–ù–´–ï:")
    print("‚îÄ" * 40)
    
    print(f"   –ù–æ–º–µ—Ä –¥–µ–ª–∞: {case_data.case_number or '–Ω–µ –Ω–∞–π–¥–µ–Ω'}")
    print(f"   –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞: {case_data.document_type or '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω'}")
    print(f"   –î–∞—Ç–∞: {case_data.date or '–Ω–µ –Ω–∞–π–¥–µ–Ω–∞'}")
    print(f"   –ì–æ—Ä–æ–¥: {case_data.city or '–Ω–µ –Ω–∞–π–¥–µ–Ω'}")
    print(f"   –°—É–¥: {case_data.court or '–Ω–µ –Ω–∞–π–¥–µ–Ω'}")
    print(f"   –ü—Ä–µ–¥—Å–µ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É—é—â–∏–π: {case_data.presiding_judge or '–Ω–µ –Ω–∞–π–¥–µ–Ω'}")
    
    if case_data.judges:
        print(f"   –°—É–¥—å–∏: {', '.join(case_data.judges)}")
    
    print(f"   –°–µ–∫—Ä–µ—Ç–∞—Ä—å: {case_data.secretary or '–Ω–µ –Ω–∞–π–¥–µ–Ω'}")
    
    if case_data.plaintiffs:
        print(f"\n   üìå –ò–°–¢–¶–´ ({len(case_data.plaintiffs)}):")
        for i, p in enumerate(case_data.plaintiffs, 1):
            print(f"      {i}. {p}")
    else:
        print("\n   üìå –ò—Å—Ç—Ü—ã: –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    if case_data.defendants:
        print(f"\n   üìå –û–¢–í–ï–¢–ß–ò–ö–ò ({len(case_data.defendants)}):")
        for i, d in enumerate(case_data.defendants, 1):
            print(f"      {i}. {d}")
    else:
        print("\n   üìå –û—Ç–≤–µ—Ç—á–∏–∫–∏: –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ JSON
    if save_json:
        json_path = out_dir / (input_path.stem + "_data.json")
        json_path.write_text(case_data.to_json(), encoding='utf-8')
        print(f"\n‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {json_path}")
    
    print("\n" + "=" * 60)
    
    return case_data


def process_directory(input_dir: str, output_dir: str = None) -> List[CourtCase]:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö PDF —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    
    Args:
        input_dir: –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å PDF —Ñ–∞–π–ª–∞–º–∏
        output_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    
    Returns:
        List[CourtCase]: —Å–ø–∏—Å–æ–∫ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    input_path = Path(input_dir)
    
    if not input_path.exists():
        print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {input_path}")
        sys.exit(1)
    
    pdf_files = list(input_path.glob("*.pdf"))
    
    if not pdf_files:
        print(f"‚ùå PDF —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤: {input_path}")
        sys.exit(1)
    
    print(f"\nüìÅ –ù–∞–π–¥–µ–Ω–æ {len(pdf_files)} PDF —Ñ–∞–π–ª–æ–≤")
    
    results = []
    for pdf_file in pdf_files:
        try:
            case_data = process_pdf(str(pdf_file), output_dir)
            results.append(case_data)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {pdf_file.name}: {e}")
    
    # –°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "=" * 60)
    print("üìä –°–í–û–î–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("=" * 60)
    print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(results)}/{len(pdf_files)}")
    print(f"   –ù–∞–π–¥–µ–Ω–æ –∏—Å—Ç—Ü–æ–≤: {sum(len(c.plaintiffs) for c in results)}")
    print(f"   –ù–∞–π–¥–µ–Ω–æ –æ—Ç–≤–µ—Ç—á–∏–∫–æ–≤: {sum(len(c.defendants) for c in results)}")
    print(f"   –ù–∞–π–¥–µ–Ω–æ —Å—É–¥–µ–π: {sum(1 for c in results if c.presiding_judge)}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—â–µ–≥–æ JSON
    if output_dir:
        out_dir = Path(output_dir)
    else:
        out_dir = input_path
    
    summary_path = out_dir / "all_cases_summary.json"
    summary_data = [case.to_dict() for case in results]
    summary_path.write_text(json.dumps(summary_data, ensure_ascii=False, indent=2), encoding='utf-8')
    print(f"\n‚úÖ –°–≤–æ–¥–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {summary_path}")
    
    return results


# ============================================================
# –ù–ê–°–¢–†–û–ô–ö–ò - –£–ö–ê–ñ–ò–¢–ï –ü–£–¢–¨ –ö –§–ê–ô–õ–£ –ó–î–ï–°–¨
# ============================================================

INPUT_PATH = r"C:\Users\adelmatov001\court_project\docs\republic\supreme\2025\6001-25-00-6–∞–ø_21\2025-05-23_21_–¢–û–û_SANDALYECI-_–ø—Ä–æ–µ–∫—Ç.pdf"

# –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (None = —Ä—è–¥–æ–º —Å –∏—Å—Ö–æ–¥–Ω—ã–º —Ñ–∞–π–ª–æ–º)
OUTPUT_DIR = None

# –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ —Ñ–∞–π–ª—ã
SAVE_TXT = True
SAVE_JSON = True

# ============================================================
# –¢–û–ß–ö–ê –í–•–û–î–ê
# ============================================================

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å: –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ –∏–ª–∏ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π INPUT_PATH
    if len(sys.argv) > 1:
        input_path = sys.argv[1]
    else:
        input_path = INPUT_PATH
    
    # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    save_txt = SAVE_TXT if "--no-txt" not in sys.argv else False
    save_json = SAVE_JSON if "--no-json" not in sys.argv else False
    
    output_dir = OUTPUT_DIR
    if "--output" in sys.argv:
        idx = sys.argv.index("--output")
        if idx + 1 < len(sys.argv):
            output_dir = sys.argv[idx + 1]
    
    path = Path(input_path)
    
    if not path.exists():
        print(f"‚ùå –ü—É—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω: {input_path}")
        print("\nüí° –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É:")
        print("   1. –í –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π INPUT_PATH –≤ –∫–æ–¥–µ")
        print("   2. –ò–ª–∏ —á–µ—Ä–µ–∑ –∫–æ–º–∞–Ω–¥–Ω—É—é —Å—Ç—Ä–æ–∫—É: python script.py path/to/file.pdf")
        sys.exit(1)
    
    if path.is_file() and path.suffix.lower() == '.pdf':
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        process_pdf(input_path, output_dir, save_txt, save_json)
    elif path.is_dir():
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        process_directory(input_path, output_dir)
    else:
        print(f"‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –ø—É—Ç—å: {input_path}")
        print("   –£–∫–∞–∂–∏—Ç–µ PDF —Ñ–∞–π–ª –∏–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å PDF —Ñ–∞–π–ª–∞–º–∏")
        sys.exit(1)


if __name__ == "__main__":
    main()